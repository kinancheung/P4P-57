MobileNet models
0.04521825054181759
broadcasting
0.03196423153129472
generations
0.030145766810069946
XNNPACK
0.026843337999454486
Android
0.023341499767688408
ms Pixel
0.022509382320120307
ArXiv
0.02214307613387572
Linux
0.01968537865277467
Pixel phones
0.019576406420266146
floating-point neural network inference operators
0.017252120318192843
Publications

Marat Dukhan
0.0160934567364064
Paper
0.016061203847724262
inputs
0.0156739375686474
integer
0.01363096217032988
1.0X
0.013434766349484076
WatchOS
0.01296033331582376
iOS
0.012867670777866401
ARM64
0.01223524525848163
channels
0.011817423319836554
subset
0.011557835463367588
PyTorch
0.011502423763687772
MediaPipe
0.011216438735265474
x86 platforms
0.011102433720046427
TensorFlow Lite
0.010867774011899646
randomized weights
0.010690723939091613
operators
0.010642642370121384
input tensor
0.010624823077016069
Raspberry Pi

The table
0.010535307606294283
ms
MobileNet v1
0.010223013181888617
single-threaded performance
0.010141805691779665
table
0.009418932086570754
low-level performance
0.009204671590688359
XNNPACK

XNNPACK
0.008043130734763877
tvOS
0.0076558037790643
QNNPACK library
0.00764867112351494
Architectures
0.007496717854093177
workshop
0.0074225958986546825
macOS
0.007360553277603751
ARMv7
0.007329499289633584
slides, paper
0.007254104151772359
TensorFlow.js
0.0072522153035631635
NEON
0.007247325110148459
ReLU
0.007162793596609278
Windows
0.00715191111820401
ReLU6
0.007091573217490925
ECV
0.0070291393992384255
Trevor Gale
0.006982183724913078
Minimum
0.006936376224876649
Karen Simonyan
0.006915903878818094
Maximum
0.0068917812113683495
Multiply
0.006889055035875796
Erich Elsen
0.006758643456673399
Divide
0.0067459455901249755
Compute Vision
0.006664131389178611
pre-trained sparse
models
0.006598678707552108
Clamp
0.006588947130116022
Squared Difference
0.006555249087728036
Fast Sparse ConvNets
0.006531342482808392
AVX512
0.0063726469625226875
WebAssembly
0.006352224818282879
Yury Pisarchyk
0.006329353537607608
Subtract
0.006274028925389842
Copy
Floor
0.006224864356612046
codebase
0.006224786485639338
Abs
0.006167097878854031
absolute value
0.006129728236940291
Artsiom Ablavatski
0.006088970806809527
time
0.006082175442298616
Efficient Deep Learning
0.006056453249916784
deep learning practitioners
0.006044473127359943
frameworks
0.006041937947961742
XNNPACK support NHWC layout
0.006036993970343095
Global Average Pooling
Channel Shuffle
0.005995944930293163
ARM
0.0059657886173581035
direct use
0.005939729628254795
Ceiling
0.005938049326213483
Large
0.005914525692603335
Channel dimension
0.005908011056050257
Juhyun Lee "Efficient Memory Management
0.005901389059953946
Zero W
0.005816095400787342
BCM2835
0.005782468521367308
researchers
0.005740986514531082
Two-Pass Softmax Algorithm
0.005738268674495972
custom stride
0.0057163034343693684
BCM2836
0.005643070702717117
Indirect Convolution Algorithm
0.0056292300190407965
lot
0.005616629810407241
simulator
WebAssembly MVP
WebAssembly SIMD
0.005597393379260971
big cores) performance
0.005515165741532147
ties
0.005514481570861042
end2end_bench --benchmark_min_time=5
0.005510679783667544
Raspbian Buster
0.005479979191306216
Ecosystem

Machine Learning Frameworks

TensorFlow.js WebAssembly backend
0.005436818369593382
CMake
0.005417988042437771
Performance

Mobile phones
0.0053647606323445444
cost Channel Split and Channel Concatenation operations
0.005336057196953834
android_arm64 :end2end_bench
0.0053056394638426665
end2end-bench --benchmark_min_time=5
0.005288310533686762
March
0.005265807812827839
BCM2837B0
0.005265642223100418
Deep Neural Net Inference
0.005250441478976969
2D Convolution
0.005247974390677795
many threads
0.005235261425253047
grouped and depthwise)
2D Deconvolution
0.005217291365914196
May
0.005047591970960511
Square
Truncation
0.00501875076313788
BCM2711
0.004999568831571386
AKA Transposed Convolution
0.00499919162046273
optimized library
0.004813966177733499
2D Average Pooling
2D Max Pooling
0.004811464046653786
